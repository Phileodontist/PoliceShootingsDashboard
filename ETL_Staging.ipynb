{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.ini']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import configparser\n",
    "import boto3 as b3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from sql_queries import create_stage_table_queries, drop_stage_table_queries\n",
    "from schemas import stage_police_shootings_schema, stage_us_cities_schema, \\\n",
    "                    stage_us_demographics_schema, stage_unemployment_schema, raw_unemployment_schema\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.postgresql:postgresql:42.1.1 pyspark-shell'\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Creates a Spark Sessions\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "                        .config(\"spark.driver.extraClassPath\", os.environ['PYSPARK_SUBMIT_ARGS']) \\\n",
    "                        .master('local[*]') \\\n",
    "                        .appName('ETL')\\\n",
    "                        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_unemployment_data(state_id):\n",
    "    \"\"\"\n",
    "    Requests data for each state\n",
    "    \n",
    "    Return response from unemployment API\n",
    "    \"\"\"\n",
    "    endPointTemplate = 'https://api.careeronestop.org/v1/unemployment/{}/{}/{}'\n",
    "    \n",
    "    headersAuth = {\n",
    "        'Authorization': 'Bearer '+ config.get('unemploymentAPI', 'unemployment_api_key')\n",
    "    }\n",
    "    \n",
    "    url = endPointTemplate.format(config.get('unemploymentAPI', 'unemployment_userID'), state_id, 'county')\n",
    "    response = requests.get(url, headers=headersAuth, verify=True)\n",
    "    counties = response.json()['CountyList']\n",
    "    \n",
    "    results = []\n",
    "    for county in counties:\n",
    "        county['StateID'] = state_id\n",
    "        results.append(county)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access state ids on local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_unemployment_data(spark, request_unemployment_data):\n",
    "    \"\"\"\n",
    "    Retrieve unemployment data from API\n",
    "    \n",
    "    Returns a df containing unemployment data\n",
    "    \"\"\"\n",
    "    with open(config.get('pathways', 'state_ids'), 'r') as f:\n",
    "        ids = f.readline()\n",
    "        state_ids = ids.split(',')\n",
    "        rdd = spark.sparkContext.parallelize(state_ids)\n",
    "        unemployment_data = rdd.flatMap(request_unemployment_data)\n",
    "        \n",
    "    df = spark.createDataFrame(unemployment_data, raw_unemployment_schema) \\\n",
    "              .selectExpr('AreaID as area_id', \\\n",
    "                          'AreaName as area_name', \\\n",
    "                          'AreaType as area_type', \\\n",
    "                          'Stfips as stfips', \\\n",
    "                          'UnEmpCount as unemployment_count', \\\n",
    "                          'UnEmpRate as unemployment_rate', \\\n",
    "                          'StateID as state_id')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_stage(df, db_properties, table_name):\n",
    "    \"\"\"\n",
    "    Write raw data into staging tables\n",
    "    \"\"\"\n",
    "    df.write.option('driver', 'org.postgresql.Driver').jdbc(url=config.get('postgres', 'url'), table=table_name, \\\n",
    "                  mode='overwrite', properties=db_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access other datasets on local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data(spark, schema, file_name):\n",
    "    \"\"\"\n",
    "    Read data from files in S3\n",
    "    \n",
    "    Return df containing data from files\n",
    "    \"\"\"\n",
    "    df = spark.read.option('header', 'true').option('sep', ',').schema(schema).csv('file://' + config.get('pathways', file_name))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retrieve_unemployment_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1996d75dbd00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-1996d75dbd00>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Retrieve data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0munemployment_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_unemployment_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_unemployment_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mpolice_shootings_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage_police_shootings_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'police_shootings'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mus_demographics_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage_us_demographics_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'us_demographics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retrieve_unemployment_data' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Initialize session\n",
    "    spark = create_spark_session()\n",
    "    \n",
    "    # Set up db connections\n",
    "    db_properties = {}\n",
    "    db_properties['username'] = config.get('postgres','user')\n",
    "    db_properties['password'] = config.get('postgres','password')\n",
    "    \n",
    "    # Retrieve data\n",
    "    unemployment_data = retrieve_unemployment_data(spark, request_unemployment_data)\n",
    "    police_shootings_data = retrieve_data(spark, stage_police_shootings_schema, 'police_shootings')\n",
    "    us_demographics_data = retrieve_data(spark, stage_us_demographics_schema, 'us_demographics')\n",
    "    us_cities_data = retrieve_data(spark, stage_us_cities_schema, 'us_cities')\n",
    "    \n",
    "    # Write date to staging tables\n",
    "    write_to_stage(unemployment_data, db_properties, 'stage_unemployment')\n",
    "    write_to_stage(police_shootings_data, db_properties, 'stage_police_shootings')\n",
    "    write_to_stage(us_demographics_data, db_properties, 'stage_us_demographics')\n",
    "    write_to_stage(us_cities_data, db_properties, 'stage_us_cities')\n",
    "    \n",
    "    spark.stop()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
