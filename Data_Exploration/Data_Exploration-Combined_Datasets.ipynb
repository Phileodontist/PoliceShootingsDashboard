{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration: Combining all datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/lpascual/Projects/PoliceShootingsDashboard/config.ini']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.join(os.path.dirname(os.getcwd()), 'config.ini'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType, BooleanType, FloatType\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('DataExploration').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema of the Police Shootings Dataset\n",
    "psSchema = StructType([\\\n",
    "                       StructField('id', IntegerType(), False),\n",
    "                       StructField('name', StringType(), True),\n",
    "                       StructField('date', DateType(), True),\n",
    "                       StructField('manner_of_death', StringType(), True),\n",
    "                       StructField('armed', StringType(), True),\n",
    "                       StructField('age', IntegerType(), True),\n",
    "                       StructField('gender', StringType(), True),\n",
    "                       StructField('race', StringType(), True),\n",
    "                       StructField('city', StringType(), True),\n",
    "                       StructField('state', StringType(), True),\n",
    "                       StructField('s_o_m_i', BooleanType(), True),\n",
    "                       StructField('threat_level', StringType(), True),\n",
    "                       StructField('flee', StringType(), True),\n",
    "                       StructField('body_camera', BooleanType(), True),\n",
    "                       StructField('longitude', FloatType(), True),\n",
    "                       StructField('latitude', FloatType(), True),\n",
    "                       StructField('is_geocoding_exact', BooleanType(), True)\n",
    "                        ])\n",
    "\n",
    "headersAuth = {\n",
    "    'Authorization': 'Bearer '+ config['unemploymentAPI']['unemployment_api_key']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US Cities and Counties\n",
    "usCitiesDF = spark.read.option('header', 'True').option('inferSchema', 'true').csv(config['pathways']['usCities'])\n",
    "usCitiesDF.createOrReplaceTempView('usCities')\n",
    "\n",
    "# US Demographics\n",
    "usDemoDF = spark.read.option('header', 'true').option('inferSchema', 'true').csv(config['pathways']['usDemographics'])\n",
    "usDemoDF.createOrReplaceTempView('usDemo')\n",
    "\n",
    "# US Unemployment by County\n",
    "endPointTemplate = 'https://api.careeronestop.org/v1/unemployment/{}/{}/{}'\n",
    "url = endPointTemplate.format(config['unemploymentAPI']['unemployment_userID'], 'CA', 'county')\n",
    "response = requests.get(url, headers=headersAuth, verify=True)\n",
    "response = response.json()['CountyList']\n",
    "caUEDataRDD = spark.sparkContext.parallelize(response)\n",
    "caUEDataDF = spark.read.json(caUEDataRDD)\n",
    "caUEDataDF = caUEDataDF.withColumn(\"State\", func.lit('CA'))\n",
    "caUEDataDF.createOrReplaceTempView('usUnemployment')\n",
    "\n",
    "# Police Shootings\n",
    "psDF = spark.read.option('header', 'True').schema(psSchema).csv(config['pathways']['policeShootings'])\n",
    "psDF.createOrReplaceTempView('policeShootings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Cities and Counties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usCitiesDF = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        state_id,\n",
    "        state_name,\n",
    "        county_name as county,\n",
    "        city\n",
    "    FROM usCities\n",
    "\"\"\")\n",
    "\n",
    "usCitiesDF.createOrReplaceTempView('usCitiesNorm')\n",
    "usCitiesDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and Select relative columns\n",
    "# Note: Records that have null values in the following columns: sex, race, min_age, max_age\n",
    "#       need to be filtered out. Otherwise overcounting occurs.\n",
    "#       Null in those columns, represents (Across all {sex, race, age})\n",
    "usDemoNorm = spark.sql(\"\"\"   \n",
    "                        SELECT\n",
    "                            state_name,\n",
    "                            county_name,\n",
    "                            sex,\n",
    "                            min_age,\n",
    "                            max_age,\n",
    "                            year,\n",
    "                            CASE \n",
    "                                WHEN race like 'AMERICAN INDIAN%' then 'American Indian'\n",
    "                                WHEN race like 'SOME OTHER RACE%' then 'Other'\n",
    "                                WHEN race like 'WHITE%' then 'White'\n",
    "                                WHEN race like 'ASIAN%' then 'Asian'\n",
    "                                WHEN race like 'NATIVE HAWAIIAN%' then 'Native Hawaiian'\n",
    "                                WHEN race like 'TWO OR MORE%' then 'Mixed'\n",
    "                                WHEN race like 'BLACK%' then 'African American'\n",
    "                            END as race,\n",
    "                            population\n",
    "                        FROM usDemo\n",
    "                        WHERE year = '2010' AND race is NOT NULL AND sex is NOT NULL \n",
    "                              AND min_age is NOT NULL AND max_age is NOT NULL\n",
    "                        \"\"\")\n",
    "usDemoNorm.createOrReplaceTempView('usDemoNorm')\n",
    "usDemoNorm.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Police Shootings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policeShootingsNorm = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            id,\n",
    "            name,\n",
    "            date,\n",
    "            manner_of_death,\n",
    "            armed,\n",
    "            age,\n",
    "            gender,\n",
    "            s_o_m_i,\n",
    "            threat_level,\n",
    "            flee,\n",
    "            body_camera,\n",
    "            CASE\n",
    "                WHEN race = 'A' THEN 'Asian'\n",
    "                WHEN race = 'B' THEN 'Black'\n",
    "                WHEN race = 'N' THEN 'Native'\n",
    "                WHEN race = 'H' THEN 'Hispanic'\n",
    "                WHEN race = 'W' THEN 'White'\n",
    "                WHEN race = 'O' THEN 'Other'\n",
    "                ELSE 'Not Documented'\n",
    "            END as race, \n",
    "            city, \n",
    "            state as state_id\n",
    "        FROM    \n",
    "          policeShootings\n",
    "\n",
    "\"\"\")\n",
    "policeShootingsNorm.createOrReplaceTempView('policeShootingsNorm')\n",
    "policeShootingsNorm.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caUEDataDF.createOrReplaceTempView(\"caUE\")\n",
    "caUEDataDF = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            State as state_id, \n",
    "            REPLACE(AreaName, ' County', '') as county,\n",
    "            UnEmpCount as unemployment_count,\n",
    "            UnEmpRate as unemployment_rate\n",
    "        FROM caUE\n",
    "        \"\"\")\n",
    "\n",
    "caUEDataDF.createOrReplaceTempView(\"caUE\")\n",
    "caUEDataDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## California Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        usc.state_name,\n",
    "        usc.county,\n",
    "        ps.name, \n",
    "        ps.manner_of_death,\n",
    "        ps.armed,\n",
    "        ps.age,\n",
    "        ps.gender,\n",
    "        ps.s_o_m_i,\n",
    "        ps.threat_level,\n",
    "        ps.flee,\n",
    "        ps.body_camera,\n",
    "        ps.race,\n",
    "        ue.unemployment_count,\n",
    "        ue.unemployment_rate\n",
    "    FROM policeShootingsNorm as ps\n",
    "    JOIN usCitiesNorm as usc\n",
    "    ON ps.state_id = usc.state_id and ps.city = usc.city\n",
    "    JOIN caUE as ue \n",
    "    ON ps.state_id = ue.state_id and usc.county = ue.county\n",
    "    WHERE usc.state_name = 'California'\n",
    "\"\"\").show(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
