{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Police Shootings Dashboard EMR Implementation Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developer's Note\n",
    "<hr style=\"border:1px solid black\">\n",
    "\n",
    "## **What's The Goal Of This Implementation?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis\n",
    "<hr style=\"border:1px solid black\">\n",
    "\n",
    "## **What's The Goal Of The Project?**\n",
    "The goal of this project is to build an analytics dashboard that displays the distribution of police shootings across the United States. \n",
    "By pulling data in terms of demographics and unemployment levels by state/county, the context in which these shootings occur may give a \n",
    "clearer view of the patterns of police shootings.\n",
    "\n",
    "## **Type Of Queries That Will Be Ran**\n",
    "The idea is to be able to perform roll-up statistics based on dimensions; `city`, `race`, `unemployment rate`, `etc`\n",
    "1. Pull all the data together\n",
    "2. The number of police shootings based on dimensions\n",
    "3. The number of cases of mental health related shootings based on county, state, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline\n",
    "<hr style=\"border:1px solid black\">\n",
    "\n",
    "## **How Often The Data Should Be Updated**\n",
    "With the nature of how each of the individual datasets are updated, the frequency of updates varies, where in the case of police shootings and unemployment data,\n",
    "weekly updates are expected. U.S. demographics on the other hand is derived from the U.S. census, which is conducted every 10 years. \n",
    "So demographics data may not be updated at all, similarly to the U.S. cities dataset where cities and counties rarely change.\n",
    "\n",
    "## **Rationale For The Use Of Spark And Airflow** \n",
    "With the pipeline designed in using pyspark, the data that is ingested and processed will leverage the distributed processing of AWS EMR. \n",
    "\n",
    "This allows faster turn out in terms of updating the production version of the data for dashboard usage.\n",
    "As police shootings data is updated on a weekly process, using Airflow to orchestrate the ingestion and integration of the data into the data model would ensure the availability of new data within the dashboard. Having a scheduled pipeline would not only achieve updated data, but also ensure data quality as quality checks are done after major ingestions and transformations. \n",
    "\n",
    "So in the case where the pipeline has to run weekly, or even more frequently, using Airflow helps running a collection of scripts at a scheduled time \n",
    "achievable without much manual work.\n",
    "\n",
    "## **Why The Current Model Was Chosen**\n",
    "The reasoning behind the use of a standard database model is due to the nature of the police shootings dataset. \n",
    "With each record being an independent event with no values to aggregate across dimensions, defining the model as such \n",
    "proved to be the simplest and effective way of representing the heterogeneous data retrieved from a variety of sources.\n",
    "\n",
    "## **Rationale For The Choice Of Tools And Technologies For The Project**\n",
    " * Spark\n",
    " * AWS S3\n",
    " * AWS RDS\n",
    " * AWS EMR\n",
    "\n",
    "In working with a large dataset, leveraging Spark helps process and transform data relatively faster as opposed to processing all the data on one machine.\n",
    "Using AWS EMR in conjunction, provides a cluster of nodes for distributed processing. In terms of storage, AWS S3 serves as the centralized space for \n",
    "the various datasets used for this project, being scalable as the size of the data increases over time. Lastly, AWS RDS provides the database management system\n",
    "needed to store data in both the staging and production stages of data ingestion. Like AWS S3, AWS RDS can scale accordingly as need dictates.\n",
    "\n",
    "\n",
    "## How The Current System Address The Following Problems\n",
    "* **If the data increased by 100x**\n",
    "> In the event where the data volume increases significantly, the usage of scalable tools such as AWS S3, RDS and EMR allows the pipeline the flexibility \n",
    ">to increase storage and processing accordingly. This would entail increasing the storage capacity of S3, and scaling horizontally for both RDS and EMR \n",
    ">by adding more nodes to the cluster, etc.\n",
    "\n",
    "* **If the pipelines were run daily by 7am**\n",
    "> Needing the pipeline to run at a specific time, Apache Airflow comes into play by providing the means of orchestrating the pipeline and scheduling it to \n",
    ">run accordingly. The need to increase storage and processing may be dependent on the volume of data that is being ingested on a daily basis.\n",
    "\n",
    "* **If the database needed to be accessed by 100+ people**\n",
    "> When the time comes where 100+ people will be accessing the database/dashboard, scaling the AWS RDS instance would be crucial in sustaining an increase of users. By horizontally scaling, the amount of queries can be off balanced between nodes to prevent bottlenecks from occurring.\n",
    "\n",
    "<hr style=\"border:1px solid black\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
